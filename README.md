This paper proposes a neural internal model-based reinforcement learning (RL) control method to address low tracking accuracy and insufficient robustness of underactuated unmanned surface vehicles (USVs) in complex environments The challenges faced in this domain often arise from model mismatches and external disturbances. Existing control methods have inherent limitations: traditional model-based approaches rely heavily on precise hydrodynamic modeling, making them ineffective in mitigating model uncertainties and external disturbance; pure RL methods, though model-free, require large amounts of data and exhibit low sample efficiency. Inspired by internal model control, this study introduces a RL method integrating simplified rigid-body dynamics predictive errors, which calculates the error vector between the actual and predicted states and feeds it into the policy network for control input decision-making. In simulations, the USV physical model incorporates ±20% uniform parameter perturbations per training round to simulate model uncertainties. Comparative tests with PID, MPC, and SAC-vanilla show the proposed method outperforms the others, verifying predictive error feedback’s comprehensive compensation for model mismatch and external disturbances, and providing a solution for high-precision path-following of underactuated USVs in complex environments.
